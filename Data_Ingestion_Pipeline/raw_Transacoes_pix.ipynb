{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1721824e-1937-4fc3-9d5b-9d5e577477e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "BASE = \"https://olinda.bcb.gov.br/olinda/servico/Pix_DadosAbertos/versao/v1/odata\"\n",
    "\n",
    "def fetch_data_and_create_dataframe(url_base, date_param_name, date_param_value):\n",
    "    \"\"\"\n",
    "    Função para buscar os dados de um único período e retornar um DataFrame do PySpark.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    initial_url = f\"{url_base}({date_param_name}=@{date_param_name})?@{date_param_name}='{date_param_value}'&$format=json\"\n",
    "    url = initial_url\n",
    "\n",
    "    while url:\n",
    "        try:\n",
    "            r = requests.get(url)\n",
    "            r.raise_for_status()\n",
    "            response_json = r.json()\n",
    "            \n",
    "            data = response_json.get('value', [])\n",
    "            all_data.extend(data)\n",
    "            \n",
    "            url = response_json.get('@odata.nextLink')\n",
    "            \n",
    "            if url:\n",
    "                print(f\"Buscando próxima página para {date_param_value}...\")\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro na requisição para {date_param_value}: {e}\")\n",
    "            return None\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"Nenhum dado encontrado para {date_param_value}. Pulando...\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    df_transacs = pd.DataFrame(all_data)\n",
    "    \n",
    "\n",
    "    df_spark_transacs = spark.createDataFrame(df_transacs)\n",
    "    \n",
    "    return df_spark_transacs\n",
    "\n",
    "\n",
    "print(\"Iniciando coleta e processamento incremental de Estatísticas de Transações Pix...\")\n",
    "\n",
    "\n",
    "for year in range(2023, 2025):\n",
    "    for month in range(1, 13):\n",
    "        month_str = f\"{year}{month:02d}\"\n",
    "        print(f\"\\nColetando dados para o mês: {month_str}\")\n",
    "\n",
    "\n",
    "        df_spark_transacs = fetch_data_and_create_dataframe(f\"{BASE}/EstatisticasTransacoesPix\", \"Database\", month_str)\n",
    "\n",
    "        if df_spark_transacs:\n",
    "            print(f\"Dados para {month_str} coletados com sucesso.\")\n",
    "            \n",
    "            df_spark_transacs.write.format(\"delta\").mode(\"append\").saveAsTable(\"estatisticas_pix.raw_data.estatisticas_transacoes_pix\")\n",
    "            \n",
    "            print(f\"Dados de estatísticas de transações para {month_str} salvos com sucesso na tabela `estatisticas_pix.raw_data.estatisticas_transacoes_pix`.\")\n",
    "\n",
    "print(\"\\nProcesso ETL incremental concluído.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "raw_Transacoes_pix",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
