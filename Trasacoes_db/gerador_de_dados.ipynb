{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a008c2eb-7fac-4784-807c-7119a1a84188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5844a3af-048a-4579-8a64-5e7ef630f800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install faker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e316d1ba-aa89-4bb7-9493-a8443b000461",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f7a330-5fab-4f44-a458-4a1d75d22105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "import random\n",
    "from faker import Faker\n",
    "from datetime import datetime, timedelta, date\n",
    "from typing import Iterator, Tuple\n",
    "import calendar\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d412762-c888-4670-acc5-40398b683b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parâmetros de Configuração\n",
    "\n",
    "- **NOME_APLICACAO_SPARK**: Nome identificador desta execução no ambiente Spark.\n",
    "- **ANO_ESTATISTICA**: Ano base utilizado para buscar as estatísticas de volume de transações.\n",
    "- **FATOR_ESCALA_VOLUME**: Fator de escala para reduzir o volume total de transações geradas. Por exemplo, `0.006` indica que será gerado apenas 0,6% do volume real, útil para criar amostras menores e mais rápidas de processar.\n",
    "- **LIMITE_ABSOLUTO_TX**: Limite máximo absoluto de transações a serem geradas, evitando volumes excessivos.\n",
    "- **LIMITE_MUNICIPIOS_PROCESSADOS**: Quantidade máxima de municípios para os quais serão gerados dados, priorizando aqueles com maior volume.\n",
    "- **PROBABILIDADE_TRANSACAO_INTERMUNICIPAL**: Proporção de transações que terão como destino um município diferente do de origem (ex: 20%), tornando a simulação mais realista.\n",
    "\n",
    "---\n",
    "\n",
    "### Parâmetros de Fraude\n",
    "\n",
    "- **PROBABILIDADE_FRAUDE_BASE**: Probabilidade padrão de uma transação ser fraudulenta (ex: 5%).\n",
    "- **PROB_CONTA_ALTO_RISCO**: Probabilidade de uma conta ser classificada como \"alto risco\" ao ser criada (ex: 10%).\n",
    "- **PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO**: Probabilidade de fraude quando o destino é uma conta de alto risco (ex: 60%).\n",
    "- **PROBABILIDADE_FRAUDE_CHAVE_RECENTE**: Probabilidade de fraude quando o PIX é destinado a uma chave recém-cadastrada (ex: 40%), já que contas novas são frequentemente usadas em golpes.\n",
    "- **DIAS_CHAVE_CONSIDERADA_RECENTE**: Número de dias para considerar uma chave como \"recente\" (ex: 7 dias).\n",
    "- **MAX_DIAS_CADASTRO_CHAVE_RISCO**: Prazo máximo para contas de alto risco cadastrarem suas chaves PIX após a abertura (ex: 5 dias).\n",
    "\n",
    "---\n",
    "\n",
    "### Parâmetros Gerais\n",
    "\n",
    "- **MULTIPLICADOR_MAGNITUDE_OUTLIER**: Multiplicador para definir o valor de uma transação \"outlier\" (ex: 25 vezes o valor base).\n",
    "- **MULTIPLICADOR_MAGNITUDE_FRAUDE**: Multiplicador para definir o valor de uma transação fraudulenta (ex: 50 vezes o valor base), simulando tentativas de golpes de alto valor.\n",
    "- **PROBABILIDADES_TIPO_FRAUDE**: Dicionário que define a chance de cada tipo de fraude ocorrer. Exemplo: `valor_atipico` tem 30% de chance de ser o tipo escolhido em caso de fraude.\n",
    "- **PESO_CONTAS_POS_PIX**: Proporção de contas criadas após o lançamento do PIX (ex: 70%), refletindo o aumento da bancarização no período."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55ad0bfd-3b2c-49a8-b619-aa39da66a56e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. PARÂMETROS GLOBAIS DE CONFIGURAÇÃO\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"INFO: Definindo parâmetros globais...\")\n",
    "NOME_APLICACAO_SPARK = \"GeradorDadosPix_Absoluto_v10.23_LogicaAvancada\"\n",
    "\n",
    "# === PARÂMETROS DE ESCALA E VOLUME ===\n",
    "FATOR_ESCALA_VOLUME = 0.3 # Ajustado para 1% do volume estatístico base\n",
    "LIMITE_ABSOLUTO_TX = 20000 \n",
    "LIMITE_MUNICIPIOS_PROCESSADOS = 10\n",
    "TX_POR_CLIENTE_ESPERADO = 15.0 # Ajustado para um valor mais realista de transações por cliente/mês\n",
    "PROBABILIDADE_TRANSACAO_INTERMUNICIPAL = 0.20 \n",
    "\n",
    "# === PARÂMETROS DE RISCO E CONTA (Para Geração de Clientes/Contas) ===\n",
    "PROB_CONTA_ALTO_RISCO = 0.03 # Ajustado para refletir a raridade de contas de alto risco (3%)\n",
    "PESO_CONTAS_POS_PIX = 0.70 # Probabilidade de a conta ter sido aberta após o lançamento do Pix (jan/2020)\n",
    "\n",
    "# === PARÂMETROS DE FRAUDE (Bernoulli Condicional) ===\n",
    "PROBABILIDADE_FRAUDE_BASE = 0.001 # Ajustado drasticamente para 0.1% (taxa base de fraude mais realista)\n",
    "PROBABILIDADE_OUTLIER_BENIGNO = 0.04 # Taxa de outliers legítimos\n",
    "MULTIPLICADOR_MAGNITUDE_OUTLIER = 25 # Mantido para garantir que outliers sejam detectáveis\n",
    "MULTIPLICADOR_MAGNITUDE_FRAUDE = 50 # Mantido para garantir que a fraude seja de alta magnitude\n",
    "\n",
    "# Parâmetros Específicos da Lógica de Fraude\n",
    "PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO = 0.60 \n",
    "PROBABILIDADE_FRAUDE_CHAVE_RECENTE = 0.40      \n",
    "DIAS_CHAVE_CONSIDERADA_RECENTE = 7             \n",
    "MAX_DIAS_CADASTRO_CHAVE_RISCO = 5              \n",
    "\n",
    "# === PROBABILIDADES DE TIPO DE FRAUDE (Distribuição Categórica) ===\n",
    "PROBABILIDADES_TIPO_FRAUDE = {\n",
    "    \"valor_atipico\": 0.30, \n",
    "    \"engenharia_social\": 0.25, \n",
    "    \"tomada_de_conta\": 0.15, \n",
    "    \"triangulacao_conta_laranja\": 0.15, \n",
    "    \"fraude_qr_code\": 0.10, \n",
    "    \"ataque_de_frequencia\": 0.05\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c098b61a-0d3b-4791-8acc-95c85081808b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicialização do Ambiente e Esquemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d762d55c-9a11-44a3-9ec2-ff5e2af59c21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# INFO: Inicializando Spark Session e Faker\n",
    "\n",
    "fake = Faker('pt_BR')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "spark = SparkSession.builder.appName(NOME_APLICACAO_SPARK).getOrCreate()\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "print(\"INFO: Definindo esquemas explícitos para UDFs e tabelas Delta...\")\n",
    "\n",
    "SCHEMA_CLIENTES_UDF = StructType([\n",
    "    StructField(\"nome\", StringType()),\n",
    "    StructField(\"registro_nacional\", StringType()),\n",
    "    StructField(\"nascido_em\", DateType())\n",
    "])\n",
    "\n",
    "SCHEMA_CONTAS_UDF = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"saldo\", DoubleType()),\n",
    "    StructField(\"aberta_em\", DateType()),\n",
    "    StructField(\"agencia\", StringType()),\n",
    "    StructField(\"numero\", StringType()),\n",
    "    StructField(\"id_tipo_conta\", IntegerType()),\n",
    "    StructField(\"ispb_instituicao\", StringType()),\n",
    "    StructField(\"id_cliente\", StringType())\n",
    "])\n",
    "\n",
    "SCHEMA_CHAVES_UDF = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"chave\", StringType()),\n",
    "    StructField(\"id_tipo_chave\", IntegerType()),\n",
    "    StructField(\"cadastrada_em\", DateType()),\n",
    "    StructField(\"id_conta\", StringType())\n",
    "])\n",
    "\n",
    "SCHEMA_TRANSACOES_UDF = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"valor\", DoubleType()),\n",
    "    StructField(\"data\", TimestampType()),\n",
    "    StructField(\"mensagem\", StringType()),\n",
    "    StructField(\"id_conta_origem\", StringType()),\n",
    "    StructField(\"id_conta_destino\", StringType()),\n",
    "    StructField(\"id_tipo_iniciacao_pix\", IntegerType()),\n",
    "    StructField(\"id_finalidade_pix\", IntegerType()),\n",
    "    StructField(\"is_fraud\", IntegerType()),\n",
    "    StructField(\"fraud_type\", StringType()),\n",
    "    StructField(\"id_transacao_cadeia_pai\", StringType())\n",
    "])\n",
    "\n",
    "SCHEMA_CLIENTES_FINAL = StructType([\n",
    "    StructField(\"id\", StringType(), False),\n",
    "    StructField(\"nome\", StringType()),\n",
    "    StructField(\"id_natureza\", IntegerType()),\n",
    "    StructField(\"registro_nacional\", StringType()),\n",
    "    StructField(\"nascido_em\", DateType()),\n",
    "    StructField(\"estado_ibge\", IntegerType()),\n",
    "    StructField(\"municipio_ibge\", IntegerType())\n",
    "])\n",
    "\n",
    "SCHEMA_CONTAS_FINAL = StructType(\n",
    "    SCHEMA_CONTAS_UDF.fields + [\n",
    "        StructField(\"is_high_risk\", IntegerType()),\n",
    "        StructField(\"estado_ibge\", IntegerType()),\n",
    "        StructField(\"municipio_ibge\", IntegerType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "SCHEMA_CHAVES_PIX_FINAL = StructType(\n",
    "    SCHEMA_CHAVES_UDF.fields + [\n",
    "        StructField(\"estado_ibge\", IntegerType()),\n",
    "        StructField(\"municipio_ibge\", IntegerType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "SCHEMA_TRANSACOES_FINAL = StructType(\n",
    "    SCHEMA_TRANSACOES_UDF.fields + [\n",
    "        StructField(\"estado_ibge\", IntegerType())\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# AGRUPAMENTO DE CONFIGURAÇÃO\n",
    "\n",
    "config_geracao = {\n",
    "    \"PROBABILIDADE_FRAUDE_BASE\": PROBABILIDADE_FRAUDE_BASE,\n",
    "    \"PROB_CONTA_ALTO_RISCO\": PROB_CONTA_ALTO_RISCO,\n",
    "    \"PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO\": PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO,\n",
    "    \"PROBABILIDADE_FRAUDE_CHAVE_RECENTE\": PROBABILIDADE_FRAUDE_CHAVE_RECENTE,\n",
    "    \"DIAS_CHAVE_CONSIDERADA_RECENTE\": DIAS_CHAVE_CONSIDERADA_RECENTE,\n",
    "    \"MAX_DIAS_CADASTRO_CHAVE_RISCO\": MAX_DIAS_CADASTRO_CHAVE_RISCO,\n",
    "    \"MULTIPLICADOR_MAGNITUDE_OUTLIER\": MULTIPLICADOR_MAGNITUDE_OUTLIER,\n",
    "    \"MULTIPLICADOR_MAGNITUDE_FRAUDE\": MULTIPLICADOR_MAGNITUDE_FRAUDE,\n",
    "    \"PROBABILIDADES_TIPO_FRAUDE\": PROBABILIDADES_TIPO_FRAUDE,\n",
    "    \"PESO_CONTAS_POS_PIX\": PESO_CONTAS_POS_PIX\n",
    "}\n",
    "\n",
    "\n",
    "# 6. CARREGAMENTO DE DADOS AUXILIARES\n",
    "\n",
    "print(\"INFO: Carregando dados de dimensão e estatísticas...\")\n",
    "try:\n",
    "    instituicoes_pd = spark.table(\"transacoes_db.copper.instituicoes\").toPandas()\n",
    "    tipos_conta_pd = spark.table(\"transacoes_db.copper.tipos_conta\").toPandas()\n",
    "    perfis_de_uso_dict = spark.table(\"transacoes_db.pix_baseline_metricas.perfil_de_usuarios\").toPandas().to_dict('records')\n",
    "    LISTA_ISPBS_LOCAL = instituicoes_pd['ispb'].tolist()\n",
    "    LISTA_TIPOS_CONTA_LOCAL = tipos_conta_pd['id'].tolist()\n",
    "    print(\"INFO: Dados auxiliares e estatísticas carregados com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERRO: Falha ao carregar dados de dimensão ou estatísticas. Erro: {e}\"); raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7511c2-b2cb-4d6b-843c-22ae96a527c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Funções de Geração e Salvamento\n",
    "\n",
    "As funções abaixo trabalham em conjunto para criar a população e as transações de um município:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `_gerar_clientes`, `_gerar_contas`, `_gerar_chaves_pix`\n",
    "\n",
    "- **_gerar_clientes**  \n",
    "  Cria o número especificado de Pessoas Físicas (PF) e Jurídicas (PJ), gerando IDs, nomes, CPFs/CNPJs e datas de nascimento/fundação.\n",
    "\n",
    "- **_gerar_contas**  \n",
    "  Para cada cliente, cria um número aleatório de contas bancárias.\n",
    "\n",
    "- **_gerar_chaves_pix**  \n",
    "  Para cada conta, gera uma chave PIX (CPF, e-mail, telefone, etc.), garantindo que a data de cadastro da chave seja sempre posterior à data de abertura da conta.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `_gerar_detalhes_transacao_python_vetorizado`\n",
    "\n",
    "Função responsável por criar os detalhes de cada transação:\n",
    "\n",
    "- **Data da Transação:**  \n",
    "  Seleciona uma data e hora aleatória dentro do mês de referência.\n",
    "\n",
    "- **Lógica de Fraude:**  \n",
    "  Define a probabilidade de fraude com base em regras, como se a conta destino é de risco ou se a chave é recente.\n",
    "\n",
    "- **Valor por Tipo de Conta:**  \n",
    "  Gera um valor monetário condizente com o tipo de transação (ex: salário, transferência para poupança, etc.).\n",
    "\n",
    "- **Multiplicadores de Fraude e Outlier:**  \n",
    "  Aumenta drasticamente o valor da transação se ela for fraudulenta ou um outlier.\n",
    "\n",
    "- **Fraudes em Cadeia:**  \n",
    "  Para certos tipos de fraude, divide o valor inicial em várias subtransações menores, simulando lavagem de dinheiro.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `gerar_transacoes`\n",
    "\n",
    "Orquestra a geração das transações para um município em um determinado mês:\n",
    "\n",
    "- **Divisão do Volume:**  \n",
    "  Separa o total de transações em locais e intermunicipais.\n",
    "\n",
    "- **Geração de Pares Locais:**  \n",
    "  Sorteia aleatoriamente pares de contas (origem e destino) do mesmo município.\n",
    "\n",
    "- **Geração de Pares Intermunicipais:**  \n",
    "  Sorteia contas de origem do município atual e contas de destino de outros municípios.\n",
    "\n",
    "- **União e Enriquecimento:**  \n",
    "  Junta os dois conjuntos de pares e busca as informações necessárias para gerar os detalhes de cada transação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d31bc5ee-a950-4064-8ec2-d688a095b1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(SCHEMA_CLIENTES_UDF)\n",
    "def _gerar_detalhes_cliente_udf(id_natureza: pd.Series) -> pd.DataFrame:\n",
    "    local_fake = Faker('pt_BR')\n",
    "    resultados = []\n",
    "    for natureza in id_natureza:\n",
    "        if natureza == 1:\n",
    "            nascido_em = local_fake.date_of_birth(minimum_age=18, maximum_age=80)\n",
    "            resultados.append({\n",
    "                \"nome\": local_fake.name(),\n",
    "                \"registro_nacional\": local_fake.cpf(),\n",
    "                \"nascido_em\": nascido_em\n",
    "            })\n",
    "        else:\n",
    "            nascido_em = local_fake.date_between(start_date='-20y', end_date='-1y')\n",
    "            resultados.append({\n",
    "                \"nome\": local_fake.company(),\n",
    "                \"registro_nacional\": local_fake.cnpj(),\n",
    "                \"nascido_em\": nascido_em\n",
    "            })\n",
    "    return pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_clientes(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int) -> DataFrame:\n",
    "    df_base_pf = spark.range(num_pf).withColumn(\"id_natureza\", F.lit(1))\n",
    "    df_base_pj = spark.range(num_pj).withColumn(\"id_natureza\", F.lit(2))\n",
    "    df_base_clientes = df_base_pf.union(df_base_pj)\n",
    "    return (\n",
    "        df_base_clientes\n",
    "        .withColumn(\"id\", F.expr(\"uuid()\"))\n",
    "        .withColumn(\"detalhes\", _gerar_detalhes_cliente_udf(F.col(\"id_natureza\")))\n",
    "        .select(\n",
    "            \"id\",\n",
    "            F.col(\"detalhes.nome\").alias(\"nome\"),\n",
    "            \"id_natureza\",\n",
    "            F.col(\"detalhes.registro_nacional\").alias(\"registro_nacional\"),\n",
    "            F.col(\"detalhes.nascido_em\").alias(\"nascido_em\"),\n",
    "            F.lit(estado_ibge).alias(\"estado_ibge\"),\n",
    "            F.lit(municipio_ibge).alias(\"municipio_ibge\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "def _gerar_detalhes_conta_python(\n",
    "    iterator: Iterator[pd.DataFrame], config: dict, tipos_conta: list, ispbs: list\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    local_fake = Faker('pt_BR')\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            if row.id_natureza == 1:\n",
    "                saldo = round(np.random.lognormal(mean=6, sigma=1.5), 2)\n",
    "                tipo_conta = random.choice(tipos_conta)\n",
    "            else:\n",
    "                saldo = round(np.random.lognormal(mean=9, sigma=1.8), 2)\n",
    "                tipo_conta = random.choice([c for c in tipos_conta if c in [1, 3]])\n",
    "            if random.random() < config['PESO_CONTAS_POS_PIX']:\n",
    "                aberta_em = local_fake.date_between(start_date='-3y', end_date='-1M')\n",
    "            else:\n",
    "                aberta_em = local_fake.date_between(start_date='-10y', end_date='-3y')\n",
    "            resultados.append({\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"saldo\": saldo,\n",
    "                \"aberta_em\": aberta_em,\n",
    "                \"agencia\": local_fake.numerify('####'),\n",
    "                \"numero\": local_fake.numerify('#####-#'),\n",
    "                \"id_tipo_conta\": tipo_conta,\n",
    "                \"ispb_instituicao\": random.choice(ispbs),\n",
    "                \"id_cliente\": row.id_cliente\n",
    "            })\n",
    "        yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_contas(df_clientes: DataFrame, estado_ibge: int, municipio_ibge: int) -> DataFrame:\n",
    "    df_clientes_com_num_contas = df_clientes.withColumn(\n",
    "        \"num_contas\",\n",
    "        F.when(F.col(\"id_natureza\") == 1, F.floor(F.rand() * 2) + 1)\n",
    "        .otherwise(F.floor(F.rand() * 5) + 1)\n",
    "    )\n",
    "    df_contas_base = df_clientes_com_num_contas.select(\n",
    "        F.col(\"id\").alias(\"id_cliente\"),\n",
    "        \"id_natureza\",\n",
    "        F.explode(F.sequence(F.lit(1), F.col(\"num_contas\")))\n",
    "    )\n",
    "    gerador_com_contexto = functools.partial(\n",
    "        _gerar_detalhes_conta_python,\n",
    "        config=config_geracao,\n",
    "        tipos_conta=LISTA_TIPOS_CONTA_LOCAL,\n",
    "        ispbs=LISTA_ISPBS_LOCAL\n",
    "    )\n",
    "    return (\n",
    "        df_contas_base\n",
    "        .mapInPandas(gerador_com_contexto, schema=SCHEMA_CONTAS_UDF)\n",
    "        .withColumn(\"estado_ibge\", F.lit(estado_ibge))\n",
    "        .withColumn(\"municipio_ibge\", F.lit(municipio_ibge))\n",
    "    )\n",
    "\n",
    "def _gerar_detalhes_chave_udf(iterator: Iterator[pd.DataFrame], config: dict) -> Iterator[pd.DataFrame]:\n",
    "    local_fake = Faker('pt_BR')\n",
    "    for lote in iterator:\n",
    "        resultados = []\n",
    "        for row in lote.itertuples(index=False):\n",
    "            try:\n",
    "                data_abertura_obj = pd.to_datetime(row.aberta_em).date()\n",
    "                if hasattr(row, 'is_high_risk') and row.is_high_risk == 1:\n",
    "                    dias_para_cadastrar = random.randint(1, config['MAX_DIAS_CADASTRO_CHAVE_RISCO'])\n",
    "                else:\n",
    "                    dias_para_cadastrar = random.randint(1, 90)\n",
    "                cadastrada_em = data_abertura_obj + timedelta(days=dias_para_cadastrar)\n",
    "                if row.id_natureza == 1:\n",
    "                    tipos_possiveis = {\n",
    "                        1: row.registro_nacional,\n",
    "                        2: local_fake.email(),\n",
    "                        3: local_fake.phone_number(),\n",
    "                        4: str(uuid.uuid4())\n",
    "                    }\n",
    "                else:\n",
    "                    tipos_possiveis = {\n",
    "                        5: row.registro_nacional,\n",
    "                        2: local_fake.company_email(),\n",
    "                        4: str(uuid.uuid4())\n",
    "                    }\n",
    "                tipo_chave = random.choice(list(tipos_possiveis.keys()))\n",
    "                resultados.append({\n",
    "                    \"id\": str(uuid.uuid4()),\n",
    "                    \"chave\": tipos_possiveis[tipo_chave],\n",
    "                    \"id_tipo_chave\": tipo_chave,\n",
    "                    \"cadastrada_em\": cadastrada_em,\n",
    "                    \"id_conta\": row.id_conta\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"ERRO NA GERAÇÃO DE CHAVES: {e}, DADOS: {row}\")\n",
    "        if resultados:\n",
    "            yield pd.DataFrame(resultados)\n",
    "\n",
    "def _gerar_chaves_pix(\n",
    "    df_contas_completo: DataFrame,\n",
    "    df_clientes_completo: DataFrame,\n",
    "    estado_ibge: int,\n",
    "    municipio_ibge: int\n",
    ") -> DataFrame:\n",
    "    df_contas_com_cliente = (\n",
    "        df_contas_completo\n",
    "        .join(df_clientes_completo, df_contas_completo.id_cliente == df_clientes_completo.id, \"inner\")\n",
    "        .select(\n",
    "            df_contas_completo.id.alias(\"id_conta\"),\n",
    "            \"id_natureza\",\n",
    "            \"registro_nacional\",\n",
    "            \"aberta_em\",\n",
    "            df_contas_completo.is_high_risk\n",
    "        )\n",
    "    )\n",
    "    gerador_com_contexto = functools.partial(_gerar_detalhes_chave_udf, config=config_geracao)\n",
    "    return (\n",
    "        df_contas_com_cliente\n",
    "        .mapInPandas(gerador_com_contexto, schema=SCHEMA_CHAVES_UDF)\n",
    "        .withColumn(\"estado_ibge\", F.lit(estado_ibge))\n",
    "        .withColumn(\"municipio_ibge\", F.lit(municipio_ibge))\n",
    "    )\n",
    "\n",
    "def _obter_params_tempo(ano: int, mes: int) -> tuple:\n",
    "    # Simula a função original para obter o período de tempo\n",
    "    import calendar\n",
    "    from datetime import datetime\n",
    "    num_dias = calendar.monthrange(ano, mes)[1]\n",
    "    primeiro_dia = datetime(ano, mes, 1)\n",
    "    ultimo_dia = datetime(ano, mes, num_dias, 23, 59, 59)\n",
    "    return primeiro_dia, (ultimo_dia - primeiro_dia).total_seconds()\n",
    "\n",
    "def _gerar_detalhes_transacao_python_vetorizado(\n",
    "    iterator: Iterator[pd.DataFrame], ano: int, mes: int, config: dict, perfis_uso: list\n",
    ") -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Função de User Defined Function (UDF) do Spark para Pandas que gera\n",
    "    detalhes da transação, incluindo a lógica de fraude multi-camadas.\n",
    "    \"\"\"\n",
    "    primeiro_dia, delta_segundos = _obter_params_tempo(ano, mes)\n",
    "    local_fake = Faker('pt_BR') # Usado apenas para contexto, não na lógica principal\n",
    "\n",
    "    for lote in iterator:\n",
    "        n = len(lote)\n",
    "        if n == 0:\n",
    "            continue\n",
    "\n",
    "        # --- LÓGICA VETORIZADA ORIGINAL (CALCULA is_fraud E valor) ---\n",
    "\n",
    "        lote['data'] = primeiro_dia + pd.to_timedelta(np.random.uniform(0, delta_segundos, n), unit='s')\n",
    "        \n",
    "        # Lógica de Fraude\n",
    "        lote['chave_destino_cadastrada_em'] = pd.to_datetime(lote['chave_destino_cadastrada_em'])\n",
    "        delta_dias = (lote['data'] - lote['chave_destino_cadastrada_em']).dt.days\n",
    "        lote['is_high_risk'] = lote['is_high_risk'].fillna(0).astype(int)\n",
    "        \n",
    "        # Simulação de PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO, PROBABILIDADE_FRAUDE_CHAVE_RECENTE e PROBABILIDADE_FRAUDE_BASE\n",
    "        # Valores placeholder para simular a lógica de fraude dinâmica (o código original usava valores de 'config')\n",
    "        prob_fraude_risco = config.get('PROBABILIDADE_FRAUDE_CONTA_DESTINO_RISCO', 0.1)\n",
    "        prob_fraude_recente = config.get('PROBABILIDADE_FRAUDE_CHAVE_RECENTE', 0.05)\n",
    "        prob_fraude_base = config.get('PROBABILIDADE_FRAUDE_BASE', 0.001)\n",
    "        dias_chave_recente = config.get('DIAS_CHAVE_CONSIDERADA_RECENTE', 7)\n",
    "\n",
    "        prob_fraude_dinamica = np.select(\n",
    "            [\n",
    "                lote['is_high_risk'] == 1,\n",
    "                (delta_dias >= 0) & (delta_dias <= dias_chave_recente)\n",
    "            ],\n",
    "            [\n",
    "                prob_fraude_risco,\n",
    "                prob_fraude_recente\n",
    "            ],\n",
    "            default=prob_fraude_base\n",
    "        )\n",
    "        rand_event = np.random.rand(n)\n",
    "        lote['is_fraud'] = (rand_event < prob_fraude_dinamica).astype(int)\n",
    "\n",
    "        # Lógica de Valor\n",
    "        lote['id_tipo_conta_origem'] = lote['id_tipo_conta_origem'].fillna(0).astype(int)\n",
    "        lote['id_tipo_conta_destino'] = lote['id_tipo_conta_destino'].fillna(0).astype(int)\n",
    "        \n",
    "        # Simulação de MULTIPLICADOR_MAGNITUDE_FRAUDE e MULTIPLICADOR_MAGNITUDE_OUTLIER\n",
    "        multiplicador_fraude = config.get('MULTIPLICADOR_MAGNITUDE_FRAUDE', 5.0)\n",
    "        multiplicador_outlier = config.get('MULTIPLICADOR_MAGNITUDE_OUTLIER', 2.5)\n",
    "\n",
    "        mean_log_valor = np.log(150) # Valor base para simplificação\n",
    "        valores_base = np.random.lognormal(mean=mean_log_valor, sigma=0.8, size=n)\n",
    "\n",
    "        prob_outlier = 0.04\n",
    "        is_outlier = (~(lote['is_fraud'] == 1)) & (np.random.rand(n) < prob_outlier)\n",
    "        multiplicadores = np.select(\n",
    "            [lote['is_fraud'] == 1, is_outlier],\n",
    "            [multiplicador_fraude, multiplicador_outlier],\n",
    "            default=1.0\n",
    "        )\n",
    "        lote['valor'] = np.maximum(0.01, valores_base * multiplicadores).round(2)\n",
    "\n",
    "        # Seleção do Tipo de Fraude\n",
    "        probs_tipo_fraude_keys = list(config.get('PROBABILIDADES_TIPO_FRAUDE', {\"triangulacao_conta_laranja\": 0.5, \"ataque_de_frequencia\": 0.5}).keys())\n",
    "        probs_tipo_fraude_values = list(config.get('PROBABILIDADES_TIPO_FRAUDE', {\"triangulacao_conta_laranja\": 0.5, \"ataque_de_frequencia\": 0.5}).values())\n",
    "        tipos_fraude = np.random.choice(probs_tipo_fraude_keys, n, p=probs_tipo_fraude_values)\n",
    "        lote['fraud_type'] = np.where(lote['is_fraud'] == 1, tipos_fraude, None)\n",
    "        \n",
    "        # Adiciona colunas básicas\n",
    "        lote['id'] = [str(uuid.uuid4()) for _ in range(n)]\n",
    "        lote['mensagem'] = \"Pagamento via Pix\"\n",
    "        lote['id_tipo_iniciacao_pix'] = np.random.randint(1, 4, n)\n",
    "        lote['id_finalidade_pix'] = np.random.randint(1, 5, n)\n",
    "        lote['id_transacao_cadeia_pai'] = None\n",
    "\n",
    "        colunas_para_remover = [\n",
    "            col for col in [\n",
    "                'chave_destino_cadastrada_em',\n",
    "                'is_high_risk',\n",
    "                'id_tipo_conta_origem',\n",
    "                'id_tipo_conta_destino'\n",
    "            ] if col in lote.columns\n",
    "        ]\n",
    "        lote_final = lote.drop(columns=colunas_para_remover)\n",
    "\n",
    "        # --- NOVA LÓGICA DE TRIANGULAÇÃO MULTI-CAMADAS (ROOT -> 3 -> 9) ---\n",
    "        resultados_finais = []\n",
    "        for row in lote_final.itertuples(index=False):\n",
    "            if row.is_fraud and row.fraud_type in [\"triangulacao_conta_laranja\", \"ataque_de_frequencia\"]:\n",
    "                # NÍVEL 1: Transação Raiz (Recebimento na Conta Laranja)\n",
    "                id_fraude_raiz = row.id\n",
    "                resultados_finais.append(row._asdict()) # Adiciona a transação raiz\n",
    "\n",
    "                # Variáveis de Configuração para a Dispersão\n",
    "                # Define um número aleatório de subdivisões por nível (ex: 2 a 5)\n",
    "                min_subs = 2\n",
    "                max_subs = 10\n",
    "                \n",
    "                num_subs_nivel2 = random.randint(min_subs, max_subs)\n",
    "                \n",
    "                intervalo_segundos_nivel2 = (1, 1800) # 1s a 30 minutos\n",
    "                intervalo_segundos_nivel3 = (1801, 7200) # 30 min a 2 horas\n",
    "\n",
    "                # NÍVEL 2: Dispersão da Conta Laranja (num_subs_nivel2 transações)\n",
    "                valores_nivel2 = np.random.dirichlet(np.ones(num_subs_nivel2)) * row.valor\n",
    "                \n",
    "                for k in range(num_subs_nivel2):\n",
    "                    id_transacao_nivel2 = str(uuid.uuid4())\n",
    "                    id_conta_destino_nivel2 = str(uuid.uuid4()) # Nova Conta-Mula do Nível 2\n",
    "                    segundos_offset_nivel2 = random.uniform(*intervalo_segundos_nivel2)\n",
    "                    \n",
    "                    transacao_nivel2 = {\n",
    "                        \"id\": id_transacao_nivel2,\n",
    "                        \"valor\": round(max(0.01, valores_nivel2[k]), 2),\n",
    "                        \"data\": row.data + timedelta(seconds=segundos_offset_nivel2),\n",
    "                        \"mensagem\": f\"Dispersão N2 (Parte {k+1}/{num_subs_nivel2})\",\n",
    "                        \"id_conta_origem\": row.id_conta_destino, # Origem: Conta Laranja (Destino da Raiz)\n",
    "                        \"id_conta_destino\": id_conta_destino_nivel2,\n",
    "                        \"id_tipo_iniciacao_pix\": random.randint(1, 3),\n",
    "                        \"id_finalidade_pix\": random.randint(1, 4),\n",
    "                        \"is_fraud\": 1,\n",
    "                        \"fraud_type\": row.fraud_type,\n",
    "                        \"id_transacao_cadeia_pai\": id_fraude_raiz\n",
    "                    }\n",
    "                    resultados_finais.append(transacao_nivel2)\n",
    "\n",
    "                    # NÍVEL 3: Dispersão das Contas do Nível 2 (num_subs_nivel3 sub-transações cada)\n",
    "                    num_subs_nivel3 = random.randint(min_subs, max_subs)\n",
    "                    valores_nivel3 = np.random.dirichlet(np.ones(num_subs_nivel3)) * transacao_nivel2[\"valor\"]\n",
    "\n",
    "                    for l in range(num_subs_nivel3):\n",
    "                        segundos_offset_nivel3 = random.uniform(*intervalo_segundos_nivel3)\n",
    "                        \n",
    "                        transacao_nivel3 = {\n",
    "                            \"id\": str(uuid.uuid4()),\n",
    "                            \"valor\": round(max(0.01, valores_nivel3[l]), 2),\n",
    "                            \"data\": transacao_nivel2[\"data\"] + timedelta(seconds=segundos_offset_nivel3),\n",
    "                            \"mensagem\": f\"Dispersão N3 (Sub-parte {l+1}/{num_subs_nivel3})\",\n",
    "                            \"id_conta_origem\": id_conta_destino_nivel2, # Origem: Conta Destino do Nível 2\n",
    "                            \"id_conta_destino\": str(uuid.uuid4()), # Conta-Mula Final\n",
    "                            \"id_tipo_iniciacao_pix\": random.randint(1, 3),\n",
    "                            \"id_finalidade_pix\": random.randint(1, 4),\n",
    "                            \"is_fraud\": 1,\n",
    "                            \"fraud_type\": row.fraud_type,\n",
    "                            \"id_transacao_cadeia_pai\": id_transacao_nivel2 # Pai: Transação do Nível 2\n",
    "                        }\n",
    "                        resultados_finais.append(transacao_nivel3)\n",
    "            else:\n",
    "                resultados_finais.append(row._asdict())\n",
    "        \n",
    "        yield pd.DataFrame(resultados_finais)\n",
    "\n",
    "def gerar_transacoes(\n",
    "    df_contas: DataFrame,\n",
    "    volume_total: int,\n",
    "    estado_ibge: int,\n",
    "    municipio_ibge: int,\n",
    "    ano: int,\n",
    "    mes: int\n",
    ") -> DataFrame:\n",
    "    print(f\"INFO: Gerando {volume_total} transações para {municipio_ibge} ({mes}/{ano})...\")\n",
    "\n",
    "    # Prepara dados auxiliares para os joins\n",
    "    df_chaves_pix = spark.table(\"transacoes_db.copper.chaves_pix\")\n",
    "    window_chaves = Window.partitionBy(\"id_conta\").orderBy(F.col(\"cadastrada_em\").desc())\n",
    "    df_chaves_recentes_por_conta = (\n",
    "        df_chaves_pix\n",
    "        .withColumn(\"rank\", F.rank().over(window_chaves))\n",
    "        .filter(F.col(\"rank\") == 1)\n",
    "        .select(\"id_conta\", F.col(\"cadastrada_em\").alias(\"chave_destino_cadastrada_em\"))\n",
    "    )\n",
    "\n",
    "    contas_locais = (\n",
    "        df_contas\n",
    "        .select(\"id\", \"is_high_risk\", \"id_tipo_conta\")\n",
    "        .withColumnRenamed(\"id\", \"id_conta\")\n",
    "    )\n",
    "\n",
    "    # Separa o volume de transações em locais e intermunicipais\n",
    "    volume_intermunicipal = int(volume_total * PROBABILIDADE_TRANSACAO_INTERMUNICIPAL)\n",
    "    volume_local = volume_total - volume_intermunicipal\n",
    "\n",
    "    # Gera pares LOCAIS\n",
    "    df_pares_locais = spark.createDataFrame([], StructType([]))  # df vazio\n",
    "    if volume_local > 0 and contas_locais.count() > 1:\n",
    "        contas_ids_locais = contas_locais.select(\"id_conta\")\n",
    "        num_contas_locais = contas_ids_locais.count()\n",
    "        df_pares_indices_locais = (\n",
    "            spark.range(volume_local)\n",
    "            .repartition(100)\n",
    "            .withColumn(\"idx_origem\", (F.rand() * num_contas_locais).cast(\"long\"))\n",
    "            .withColumn(\"idx_destino\", (F.rand() * num_contas_locais).cast(\"long\"))\n",
    "            .filter(\"idx_origem != idx_destino\")\n",
    "        )\n",
    "        contas_numeradas_locais = contas_ids_locais.withColumn(\n",
    "            \"idx\", F.row_number().over(Window.orderBy(\"id_conta\")) - 1\n",
    "        )\n",
    "        df_pares_locais = (\n",
    "            df_pares_indices_locais\n",
    "            .join(contas_numeradas_locais.alias(\"orig\"), df_pares_indices_locais.idx_origem == F.col(\"orig.idx\"))\n",
    "            .join(contas_numeradas_locais.alias(\"dest\"), df_pares_indices_locais.idx_destino == F.col(\"dest.idx\"))\n",
    "            .select(\n",
    "                F.col(\"orig.id_conta\").alias(\"id_conta_origem\"),\n",
    "                F.col(\"dest.id_conta\").alias(\"id_conta_destino\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Gera pares INTERMUNICIPAIS\n",
    "    df_pares_intermunicipais = spark.createDataFrame([], StructType([]))\n",
    "    if volume_intermunicipal > 0:\n",
    "        contas_origem = (\n",
    "            contas_locais\n",
    "            .select(\"id_conta\")\n",
    "            .orderBy(F.rand())\n",
    "            .limit(volume_intermunicipal)\n",
    "            .withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "        )\n",
    "        contas_destino_externas = (\n",
    "            spark.table(\"transacoes_db.copper.contas\")\n",
    "            .filter(F.col(\"municipio_ibge\") != municipio_ibge)\n",
    "            .select(F.col(\"id\").alias(\"id_conta_destino\"))\n",
    "            .orderBy(F.rand())\n",
    "            .limit(volume_intermunicipal)\n",
    "            .withColumn(\"row_id\", F.monotonically_increasing_id())\n",
    "        )\n",
    "        df_pares_intermunicipais = (\n",
    "            contas_origem\n",
    "            .join(contas_destino_externas, \"row_id\")\n",
    "            .select(F.col(\"id_conta\").alias(\"id_conta_origem\"), \"id_conta_destino\")\n",
    "        )\n",
    "\n",
    "    # Une os dois conjuntos de pares\n",
    "    df_pares_total = df_pares_locais.unionByName(df_pares_intermunicipais)\n",
    "    if df_pares_total.isEmpty():\n",
    "        print(\"AVISO: Nenhum par de transação foi gerado.\")\n",
    "        return None\n",
    "\n",
    "    # Enriquece os pares com os dados necessários para a UDF\n",
    "    df_pares_enriquecidos = (\n",
    "        df_pares_total\n",
    "        .join(contas_locais.alias(\"orig\"), df_pares_total.id_conta_origem == F.col(\"orig.id_conta\"), \"left\")\n",
    "        .join(spark.table(\"transacoes_db.copper.contas\").alias(\"dest\"), df_pares_total.id_conta_destino == F.col(\"dest.id\"), \"left\")\n",
    "        .join(df_chaves_recentes_por_conta, df_pares_total.id_conta_destino == df_chaves_recentes_por_conta.id_conta, \"left\")\n",
    "        .select(\n",
    "            \"id_conta_origem\",\n",
    "            \"id_conta_destino\",\n",
    "            F.col(\"orig.id_tipo_conta\").alias(\"id_tipo_conta_origem\"),\n",
    "            F.col(\"dest.id_tipo_conta\").alias(\"id_tipo_conta_destino\"),\n",
    "            \"chave_destino_cadastrada_em\",\n",
    "            F.col(\"dest.is_high_risk\").alias(\"is_high_risk\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    gerador_com_contexto = functools.partial(\n",
    "        _gerar_detalhes_transacao_python_vetorizado,\n",
    "        ano=ano,\n",
    "        mes=mes,\n",
    "        config=config_geracao,\n",
    "        perfis_uso=perfis_de_uso_dict\n",
    "    )\n",
    "    df_transacoes_bruto = df_pares_enriquecidos.mapInPandas(gerador_com_contexto, schema=SCHEMA_TRANSACOES_UDF)\n",
    "    return df_transacoes_bruto.withColumn(\"estado_ibge\", F.lit(estado_ibge))\n",
    "\n",
    "def salvar_dataframe_em_delta(df: DataFrame, nome_tabela_completo: str, modo: str = \"append\"):\n",
    "    if df is None or df.isEmpty():\n",
    "        print(f\"AVISO: DataFrame para a tabela '{nome_tabela_completo}' está vazio.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"INFO: Salvando dados na tabela Delta: {nome_tabela_completo} (modo: {modo})...\")\n",
    "        df.write.format(\"delta\").mode(modo).saveAsTable(nome_tabela_completo)\n",
    "        print(f\"✅ SUCESSO: Dados salvos em {nome_tabela_completo}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERRO ao salvar '{nome_tabela_completo}': {e}\")\n",
    "        raise e\n",
    "\n",
    "def gerar_e_salvar_populacao(num_pf: int, num_pj: int, estado_ibge: int, municipio_ibge: int) -> DataFrame:\n",
    "    print(f\"INFO: Gerando e materializando população para {municipio_ibge} (PF: {num_pf}, PJ: {num_pj})...\")\n",
    "    df_clientes_gerado = _gerar_clientes(num_pf, num_pj, estado_ibge, municipio_ibge)\n",
    "    salvar_dataframe_em_delta(df_clientes_gerado, \"transacoes_db.copper.clientes\", modo=\"append\")\n",
    "    df_clientes_materializado = spark.table(\"transacoes_db.copper.clientes\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    df_contas_gerado = (\n",
    "        _gerar_contas(df_clientes_materializado, estado_ibge, municipio_ibge)\n",
    "        .withColumn(\"is_high_risk\", F.when(F.rand() < PROB_CONTA_ALTO_RISCO, 1).otherwise(0))\n",
    "    )\n",
    "    salvar_dataframe_em_delta(df_contas_gerado, \"transacoes_db.copper.contas\", modo=\"append\")\n",
    "    df_contas_materializado = spark.table(\"transacoes_db.copper.contas\").filter(F.col(\"municipio_ibge\") == municipio_ibge)\n",
    "    df_chaves_pix = _gerar_chaves_pix(df_contas_materializado, df_clientes_materializado, estado_ibge, municipio_ibge)\n",
    "    salvar_dataframe_em_delta(df_chaves_pix, \"transacoes_db.copper.chaves_pix\", modo=\"append\")\n",
    "    print(f\"INFO: População para o município {municipio_ibge} adicionada com sucesso.\")\n",
    "    return df_contas_materializado\n",
    "\n",
    "def limpar_tabelas_de_destino():\n",
    "    print(\"INFO: Apagando tabelas de destino para recriação...\")\n",
    "    for tabela in [\"clientes\", \"contas\", \"chaves_pix\", \"transacoes\"]:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS transacoes_db.copper.{tabela}\")\n",
    "    print(\"INFO: ✅ Limpeza concluída.\")\n",
    "\n",
    "def criar_tabelas_de_destino():\n",
    "    print(\"INFO: Criando tabelas de destino com os schemas corretos...\")\n",
    "    tabelas = {\n",
    "        \"transacoes_db.copper.clientes\": (SCHEMA_CLIENTES_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.contas\": (SCHEMA_CONTAS_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.chaves_pix\": (SCHEMA_CHAVES_PIX_FINAL, [\"estado_ibge\", \"municipio_ibge\"]),\n",
    "        \"transacoes_db.copper.transacoes\": (SCHEMA_TRANSACOES_FINAL, [\"estado_ibge\"])\n",
    "    }\n",
    "    for nome, (schema, part_cols) in tabelas.items():\n",
    "        spark.createDataFrame([], schema).write.format(\"delta\").partitionBy(part_cols).mode(\"overwrite\").saveAsTable(nome)\n",
    "        print(f\"INFO: Tabela '{nome}' criada com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e595ad3f-94b3-4f34-92ce-7ccb468eb772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Modelagem Matemática para Simulação de Dados Sintéticos\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Este documento detalha a modelagem matemática e estatística utilizada para gerar uma população e um volume de transações sintéticas que refletem o comportamento e as características de um ambiente financeiro real, em conformidade com dados estatísticos regionais (perfis de uso) e regras de negócio.\n",
    "\n",
    "A modelagem se divide em três etapas principais:\n",
    "\n",
    "1. **Dimensionamento do volume**\n",
    "2. **Dimensionamento da população**\n",
    "3. **Modelagem comportamental para a geração de transações**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dimensionamento do Volume de Transações\n",
    "\n",
    "O volume total de transações a ser gerado para o município é baseado em dados estatísticos conhecidos (média mensal de transações) e ajustado ao escopo da simulação por meio de um fator de amostragem.\n",
    "\n",
    "### 2.1. Fórmula do Volume Anual\n",
    "\n",
    "O volume total anual de transações a gerar ($V_{\\text{ano}}$) é calculado pela seguinte relação:\n",
    "\n",
    "$$\n",
    "V_{\\text{ano}} = (\\overline{T}_{\\text{mensal}} \\times 12) \\times P_{\\text{amostra}}\n",
    "$$\n",
    "\n",
    "| Termo                | Definição                                                                                  |\n",
    "|----------------------|-------------------------------------------------------------------------------------------|\n",
    "| $V_{\\text{ano}}$     | Volume anual total de transações a ser gerado.                                            |\n",
    "| $\\overline{T}_{\\text{mensal}}$ | Média estatística do número de transações de pagadores por mês no município.         |\n",
    "| $P_{\\text{amostra}}$ | Percentual da amostra desejada (`DATA_SAMPLE_PERCENTAGE`), fração do dado real a simular. |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Dimensionamento da População Sintética\n",
    "\n",
    "Com o volume de transações definido, é necessário estimar o número de clientes que compõem a população sintética, garantindo que o volume transacional seja distribuído de forma crível.\n",
    "\n",
    "### 3.1. Cálculo do Número Total de Clientes\n",
    "\n",
    "O número total de clientes a serem gerados ($N_{\\text{clientes}}$) é estimado a partir da média mensal de transações, ajustado por um fator de escala de população ($F_{\\text{escala}}$) e o percentual da amostra:\n",
    "\n",
    "$$\n",
    "N_{\\text{clientes}} = \\left\\lfloor \\frac{\\overline{T}_{\\text{mensal}}}{F_{\\text{escala}}} \\times P_{\\text{amostra}} \\right\\rfloor\n",
    "$$\n",
    "\n",
    "| Termo             | Definição                                                                                      |\n",
    "|-------------------|-----------------------------------------------------------------------------------------------|\n",
    "| $N_{\\text{clientes}}$ | Número total de clientes (Pessoa Física e Jurídica) a gerar.                                  |\n",
    "| $F_{\\text{escala}}$   | Fator de escala para relacionar volume transacional com número de clientes (`POPULATION_SCALING_FACTOR`). |\n",
    "\n",
    "### 3.2. Distribuição por Natureza (Pessoa Física e Jurídica)\n",
    "\n",
    "O total de clientes é dividido em Pessoa Física ($N_{\\text{PF}}$) e Pessoa Jurídica ($N_{\\text{PJ}}$) com base na proporção observada de transações de Pessoa Física ($\\%_{\\text{PF}}$) no município:\n",
    "\n",
    "$$\n",
    "N_{\\text{PF}} = \\lfloor N_{\\text{clientes}} \\times \\%_{\\text{PF}} \\rfloor\n",
    "$$\n",
    "\n",
    "$$\n",
    "N_{\\text{PJ}} = N_{\\text{clientes}} - N_{\\text{PF}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Modelagem Comportamental e Geração de Transações\n",
    "\n",
    "Esta seção detalha os modelos estatísticos e as regras condicionais utilizadas para gerar as transações individuais (valores, datas e características de fraude).\n",
    "\n",
    "### 4.1. Distribuição Proporcional por Perfil\n",
    "\n",
    "Para garantir que a \"mistura\" de tipos de transação no município sintético reflita a região, o volume total é distribuído proporcionalmente aos perfis de uso definidos.\n",
    "\n",
    "Primeiro, calcula-se o peso ($W_i$) de cada Perfil $i$:\n",
    "\n",
    "$$\n",
    "W_i = \\frac{Q_i}{\\sum_{j=1}^{P} Q_j}\n",
    "$$\n",
    "\n",
    "Onde $Q_i$ é a quantidade de transações do Perfil $i$ e $\\sum_{j=1}^{P} Q_j$ é a soma das quantidades de todos os perfis ($P$).\n",
    "\n",
    "Em seguida, calcula-se o volume de transações específicas para o Perfil $i$ ($V_i$):\n",
    "\n",
    "$$\n",
    "V_i = \\lfloor V_{\\text{ano}} \\times W_i \\rfloor\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2. Modelagem de Valores Monetários (Distribuição Log-Normal)\n",
    "\n",
    "A **Distribuição Log-Normal** é empregada para modelar valores monetários (saldos e valores de transação) devido à sua assimetria e natureza não negativa, o que se alinha ao comportamento de dados financeiros.\n",
    "\n",
    "Os parâmetros $\\hat{\\mu}$ (média do logaritmo) e $\\hat{\\sigma}$ (desvio padrão do logaritmo) são estimados a partir de estatísticas robustas (mediana e quartis) dos perfis para calibrar a geração:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\ln(\\text{Mediana})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma} = \\frac{\\ln(P_{75}) - \\ln(P_{25})}{1.349}\n",
    "$$\n",
    "\n",
    "O valor final da transação ($X$) é gerado a partir desta distribuição, onde o valor base é multiplicado por fatores de ajuste para simular fraudes ou valores outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3. Simulação de Eventos (Ensaio de Bernoulli e Lógica Condicional)\n",
    "\n",
    "#### A. Ensaio de Bernoulli\n",
    "\n",
    "A ocorrência de eventos discretos, como a marcação de uma transação como fraude, é modelada como um **Ensaio de Bernoulli**, onde a variável aleatória binária $X$ (sucesso/fracasso) é determinada por um número aleatório $U \\sim \\text{Uniforme}(0, 1)$ e uma probabilidade dinâmica $p$:\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{cases}\n",
    "1, & \\text{se } U < p \\ (\\text{sucesso/fraude}) \\\\\n",
    "0, & \\text{se } U \\geq p \\ (\\text{falha/normal})\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### B. Lógica Condicional Vetorizada (`np.select`)\n",
    "\n",
    "A probabilidade de fraude ($P_{\\text{fraude}}$), que define o parâmetro $p$ do Ensaio de Bernoulli, é calculada em função de múltiplas regras de negócio e características da transação (vetorizada pelo `np.select`), como demonstrado no exemplo:\n",
    "\n",
    "$$\n",
    "P_{\\text{fraude}} =\n",
    "\\begin{cases}\n",
    "P_1, & \\text{se a conta destino é de alto risco} \\\\\n",
    "P_2, & \\text{se } \\Delta_{\\text{dias}} \\leq D_{\\text{recente}} \\text{ (chave recente)} \\\\\n",
    "P_3, & \\text{caso contrário}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4.4. Simulação de Cadeias de Lavagem (Distribuição de Dirichlet)\n",
    "\n",
    "Em cenários de fraude complexa (e.g., triangulação), a **Distribuição de Dirichlet** é utilizada para simular a pulverização de um valor total em $N$ sub-transações aleatórias, garantindo que a soma das proporções seja igual a 1.\n",
    "\n",
    "O vetor de proporções $\\vec{x} = (x_1, \\dots, x_N)$ é gerado com base em parâmetros de concentração $\\alpha_i$ (tipicamente uniformes, $\\alpha_i = 1$) e aplicado ao valor da transação raiz: $V_i = x_i \\cdot V_{\\text{raiz}}$\n",
    "\n",
    "$$\n",
    "(x_1, \\dots, x_N) \\sim \\mathrm{Dirichlet}(\\alpha_1, \\dots, \\alpha_N)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^N x_i = 1\n",
    "$$\n",
    "\n",
    "O valor de cada sub-transação $V_i$ é dado por:\n",
    "\n",
    "$$\n",
    "V_i = x_i \\cdot V_{\\text{raiz}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusão\n",
    "\n",
    "A integração da **Distribuição Log-Normal**, do **Ensaio de Bernoulli**, da **Lógica Condicional Vetorizada** e da **Distribuição de Dirichlet** permite que o script de geração simule de forma eficaz um ecossistema financeiro robusto, com comportamentos e patamares de risco que se assemelham aos dados reais.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4311c855-863d-4608-a3fe-c707e29a7e66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 8. ORQUESTRAÇÃO E EXECUÇÃO PRINCIPAL\n",
    "# -----------------------------------------------------------------------------\n",
    "try:\n",
    "    limpar_tabelas_de_destino()\n",
    "    criar_tabelas_de_destino()\n",
    "    print(\"=============================================================================\")\n",
    "    print(f\"INFO: Iniciando processo de geração ANUAL (Ano: {ANO_ESTATISTICA})...\")\n",
    "    df_estatisticas_base = (\n",
    "        spark.table(\"transacoes_db.pix_baseline_metricas.relacao_pagadores_recebedores\")\n",
    "        .filter(F.col(\"Ano\") == ANO_ESTATISTICA)\n",
    "    )\n",
    "    df_volumes_anuais = (\n",
    "        df_estatisticas_base.groupBy(\n",
    "            \"cod_ibge_municipio\", \"municipio_nome\", \"cod_ibge_estado\"\n",
    "        ).agg(\n",
    "            F.sum(\"total_tx_pagador\").alias(\"volume_pagador_anual\"),\n",
    "            F.sum(\"total_tx_recebedor\").alias(\"volume_recebedor_anual\"),\n",
    "            F.sum(\"total_tx_pagador_pf\").alias(\"total_pf_anual\"),\n",
    "            F.sum(\"total_tx_pagador_pj\").alias(\"total_pj_anual\"),\n",
    "        )\n",
    "    )\n",
    "    df_ranks_anuais = (\n",
    "        df_volumes_anuais.withColumn(\n",
    "            \"rank_pagador_anual\",\n",
    "            F.rank().over(Window.orderBy(F.col(\"volume_pagador_anual\").desc())),\n",
    "        ).withColumn(\n",
    "            \"rank_recebedor_anual\",\n",
    "            F.rank().over(Window.orderBy(F.col(\"volume_recebedor_anual\").desc())),\n",
    "        )\n",
    "    )\n",
    "    df_com_rank_final = df_ranks_anuais.withColumn(\n",
    "        \"rank_combinado_anual\",\n",
    "        F.col(\"rank_pagador_anual\") + F.col(\"rank_recebedor_anual\"),\n",
    "    )\n",
    "    municipios_a_processar_lista = (\n",
    "        df_com_rank_final.orderBy(F.col(\"rank_combinado_anual\").asc())\n",
    "        .limit(LIMITE_MUNICIPIOS_PROCESSADOS)\n",
    "        .collect()\n",
    "    )\n",
    "    total_municipios = len(municipios_a_processar_lista)\n",
    "    print(f\"INFO: {total_municipios} municípios selecionados para processar.\")\n",
    "    id_municipios_selecionados = [\n",
    "        row[\"cod_ibge_municipio\"] for row in municipios_a_processar_lista\n",
    "    ]\n",
    "    df_estatisticas_filtrado = df_estatisticas_base.filter(\n",
    "        F.col(\"cod_ibge_municipio\").isin(id_municipios_selecionados)\n",
    "    )\n",
    "    volume_total_base_original = (\n",
    "        df_estatisticas_filtrado.agg(F.sum(\"total_tx_pagador\")).first()[0] or 0\n",
    "    )\n",
    "    volume_total_base_escalado = int(volume_total_base_original * FATOR_ESCALA_VOLUME)\n",
    "    if (\n",
    "        volume_total_base_original > 0\n",
    "        and volume_total_base_escalado > LIMITE_ABSOLUTO_TX\n",
    "    ):\n",
    "        fator_escala_final = LIMITE_ABSOLUTO_TX / volume_total_base_original\n",
    "    else:\n",
    "        fator_escala_final = FATOR_ESCALA_VOLUME\n",
    "    print(f\"INFO: Fator de escala final: {fator_escala_final:.8f}\")\n",
    "\n",
    "    for i, municipio_row in enumerate(municipios_a_processar_lista):\n",
    "        codigo_municipio = municipio_row[\"cod_ibge_municipio\"]\n",
    "        codigo_estado = municipio_row[\"cod_ibge_estado\"]\n",
    "        nome_municipio = municipio_row[\"municipio_nome\"]\n",
    "        print(\n",
    "            f\"\\n================== Processando Município {i+1}/{total_municipios}: {nome_municipio} ({codigo_municipio}) ==================\"\n",
    "        )\n",
    "        volume_pf_anual = int(municipio_row[\"total_pf_anual\"] * FATOR_ESCALA_VOLUME)\n",
    "        volume_pj_anual = int(municipio_row[\"total_pj_anual\"] * FATOR_ESCALA_VOLUME)\n",
    "        num_pf = max(1, int(volume_pf_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "        num_pj = max(1, int(volume_pj_anual / (TX_POR_CLIENTE_ESPERADO * 12)))\n",
    "        df_contas_do_municipio = gerar_e_salvar_populacao(\n",
    "            num_pf=num_pf,\n",
    "            num_pj=num_pj,\n",
    "            estado_ibge=codigo_estado,\n",
    "            municipio_ibge=codigo_municipio,\n",
    "        )\n",
    "        for mes in range(1, 13):\n",
    "            print(\n",
    "                f\"\\n--- Processando Mês {mes}/{ANO_ESTATISTICA} para {nome_municipio} ---\"\n",
    "            )\n",
    "            stats_mensal_row = (\n",
    "                df_estatisticas_filtrado.filter(\n",
    "                    (F.col(\"cod_ibge_municipio\") == codigo_municipio)\n",
    "                    & (F.col(\"Mes\") == mes)\n",
    "                ).first()\n",
    "            )\n",
    "            if not stats_mensal_row:\n",
    "                print(f\"AVISO: Sem estatísticas para {mes}/{ANO_ESTATISTICA}. Pulando.\")\n",
    "                continue\n",
    "            volume_total_original = stats_mensal_row[\"total_tx_pagador\"]\n",
    "            volume_total = int(volume_total_original * fator_escala_final)\n",
    "            if volume_total == 0:\n",
    "                print(\n",
    "                    f\"AVISO: Volume de transações base para {mes}/{ANO_ESTATISTICA} é 0. Pulando.\"\n",
    "                )\n",
    "                continue\n",
    "            print(\n",
    "                f\"      Volume Original: {volume_total_original} | Volume BASE Alvo: {volume_total}\"\n",
    "            )\n",
    "            try:\n",
    "                df_transacoes = gerar_transacoes(\n",
    "                    df_contas=df_contas_do_municipio,\n",
    "                    volume_total=volume_total,\n",
    "                    estado_ibge=codigo_estado,\n",
    "                    municipio_ibge=codigo_municipio,\n",
    "                    ano=ANO_ESTATISTICA,\n",
    "                    mes=mes,\n",
    "                )\n",
    "                salvar_dataframe_em_delta(\n",
    "                    df_transacoes, \"transacoes_db.copper.transacoes\", modo=\"append\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"ERRO CRÍTICO no mês {mes}/{ANO_ESTATISTICA}: {e}\")\n",
    "                import traceback\n",
    "\n",
    "                traceback.print_exc()\n",
    "finally:\n",
    "    print(\"\\nINFO: O script chegou ao fim.\")\n",
    "\n",
    "print(\"\\n=============================================================================\")\n",
    "print(\"INFO: Processo de geração de dados sintéticos concluído.\")\n",
    "print(\"=============================================================================\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7328533097160771,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "gerador_de_dados",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
